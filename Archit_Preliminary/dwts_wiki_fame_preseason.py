"""
Compute Wikipedia \"fame\" for DWTS contestants as the number of Wikipedia page edits
(revisions) up to a cutoff timestamp defined as 7 days before the season premiere date.

This script uses the English Wikipedia MediaWiki Action API:
  https://en.wikipedia.org/w/api.php

Output:
  dwts_wiki_edits_preseason.csv with columns:
    season
    celebrityname
    wiki_title
    pageid
    cutoff_utc
    edits_total_to_cutoff
    status                  # ok / missing_title / disambiguation / api_error
    notes

Resume behavior:
  The script appends to the output CSV and skips any (season, celebrityname) already present.

Dependencies (keep minimal, per requirements):
  requests, pandas, datetime, time

How to use:
  1) Ensure `Archit_Preliminary/celebrity_season_premiere_table.csv` exists
     (generated by `make_celebrity_season_premiere_table.py`).
  2) (Optional) Create `wiki_title_overrides.csv` to resolve ambiguous names.
  3) Run:
        python Archit_Preliminary/dwts_wiki_fame_preseason.py

Optional overrides:
  Place a CSV named `wiki_title_overrides.csv` in the working directory (or provide path)
  with columns: celebrityname,wiki_title
  Use this when the default title (celebrityname) is ambiguous.
"""

from __future__ import annotations

import csv
import datetime as dt
import time
from typing import Dict, Optional, Tuple

import pandas as pd
import requests


WIKI_API = "https://en.wikipedia.org/w/api.php"


def to_cutoff_utc(premiere_date_str: str) -> str:
    """
    Convert a season premiere date (YYYY-MM-DD) into a cutoff timestamp:
      cutoff = (premiere - 7 days) at 00:00:00Z

    Returns an ISO string like: '2005-05-25T00:00:00Z'
    """
    d = dt.date.fromisoformat(premiere_date_str)
    cutoff_date = d - dt.timedelta(days=7)
    cutoff_dt = dt.datetime.combine(cutoff_date, dt.time(0, 0, 0), tzinfo=dt.timezone.utc)
    return cutoff_dt.strftime("%Y-%m-%dT%H:%M:%SZ")


def _sleep_polite(seconds: float = 0.15) -> None:
    """Simple rate-limiting."""
    time.sleep(seconds)


def _requests_get_json(session: requests.Session, params: Dict, timeout: float = 20.0) -> Dict:
    """GET request with basic error handling."""
    _sleep_polite()
    resp = session.get(WIKI_API, params=params, timeout=timeout)
    resp.raise_for_status()
    return resp.json()


def resolve_pageid(
    session: requests.Session,
    title: str,
) -> Tuple[Optional[int], str, bool, str, str]:
    """
    Resolve a Wikipedia title to (pageid, normalized_title, is_disambiguation, status, notes).

    Uses:
      action=query&titles=...&redirects=1&indexpageids=1&prop=pageprops

    Disambiguation detection:
      if 'disambiguation' present in pageprops -> is_disambiguation=True
    """
    params = {
        "action": "query",
        "format": "json",
        "titles": title,
        "redirects": 1,
        "indexpageids": 1,
        "prop": "pageprops",
    }

    try:
        data = _requests_get_json(session, params)
    except Exception as exc:
        return None, title, False, "api_error", f"resolve_pageid request failed: {exc}"

    query = data.get("query", {})
    pageids = query.get("pageids", [])
    if not pageids:
        return None, title, False, "api_error", "No pageids returned by API"

    pageid_str = pageids[0]
    # Missing page
    if pageid_str == "-1":
        # Try to surface normalized title, if provided
        normalized = title
        if query.get("normalized"):
            normalized = query["normalized"][0].get("to", title)
        return None, normalized, False, "missing_title", "Title not found (pageid=-1)"

    pages = query.get("pages", {})
    page = pages.get(pageid_str, {})

    normalized_title = page.get("title", title)
    pageprops = page.get("pageprops", {}) or {}
    is_disambig = "disambiguation" in pageprops

    try:
        pageid = int(page.get("pageid"))
    except Exception:
        return None, normalized_title, is_disambig, "api_error", "Could not parse pageid"

    if is_disambig:
        return pageid, normalized_title, True, "disambiguation", "Page is a disambiguation page (pageprops.disambiguation)"

    return pageid, normalized_title, False, "ok", ""


def count_revisions_to_cutoff(
    session: requests.Session,
    pageid: int,
    cutoff_iso: str,
) -> Tuple[Optional[int], str, str]:
    """
    Count revisions for a single page up to a cutoff timestamp (inclusive).

    Uses:
      action=query
      prop=revisions
      pageids={pageid}
      rvprop=ids|timestamp
      rvlimit=max
      rvdir=older
      rvstart={cutoff_iso}

    Continuation:
      If response includes `continue`, pass those keys into the next request.

    Returns: (count or None, status, notes)
    """
    count = 0
    cont: Dict[str, str] = {}

    while True:
        params = {
            "action": "query",
            "format": "json",
            "prop": "revisions",
            "pageids": str(pageid),
            "rvprop": "ids|timestamp",
            "rvlimit": "max",
            "rvdir": "older",
            "rvstart": cutoff_iso,
        }
        params.update(cont)

        try:
            data = _requests_get_json(session, params)
        except Exception as exc:
            return None, "api_error", f"count_revisions request failed: {exc}"

        pages = data.get("query", {}).get("pages", {})
        page = pages.get(str(pageid), {})
        revs = page.get("revisions", []) or []
        count += len(revs)

        cont_data = data.get("continue")
        if not cont_data:
            break
        # The API may return rvcontinue and continue token; include all.
        cont = {k: str(v) for k, v in cont_data.items()}

    return count, "ok", ""


def _detect_col(df: pd.DataFrame, candidates) -> Optional[str]:
    """Case-insensitive detection of a column name from a list of candidates."""
    lower_map = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in lower_map:
            return lower_map[cand.lower()]
    return None


def _load_overrides(path: str) -> Dict[str, str]:
    """
    Load overrides CSV mapping celebrityname -> wiki_title.
    CSV columns: celebrityname, wiki_title
    """
    try:
        o = pd.read_csv(path)
    except FileNotFoundError:
        return {}

    name_col = _detect_col(o, ["celebrityname", "celebrity_name", "celebrity"])
    title_col = _detect_col(o, ["wiki_title", "title"])
    if not name_col or not title_col:
        raise ValueError(f"Overrides CSV {path} must have columns celebrityname and wiki_title")

    out = {}
    for _, r in o.iterrows():
        name = str(r[name_col]).strip()
        title = str(r[title_col]).strip()
        if name and title and name.lower() != "nan" and title.lower() != "nan":
            out[name] = title
    return out


def _load_existing_keys(out_csv: str) -> set[tuple[int, str]]:
    """Return set of (season, celebrityname) already written to output."""
    try:
        prev = pd.read_csv(out_csv)
    except FileNotFoundError:
        return set()
    if prev.empty:
        return set()
    season_col = _detect_col(prev, ["season"])
    name_col = _detect_col(prev, ["celebrityname", "celebrity_name", "celebrity"])
    if not season_col or not name_col:
        return set()
    keys = set()
    for _, r in prev.iterrows():
        try:
            s = int(r[season_col])
        except Exception:
            continue
        n = str(r[name_col]).strip()
        if n:
            keys.add((s, n))
    return keys


def _append_row(out_csv: str, row: Dict) -> None:
    """Append one row to CSV, writing header if file doesn't exist yet."""
    fieldnames = [
        "season",
        "celebrityname",
        "wiki_title",
        "pageid",
        "cutoff_utc",
        "edits_total_to_cutoff",
        "status",
        "notes",
    ]
    write_header = False
    try:
        with open(out_csv, "r", newline="", encoding="utf-8") as f:
            _ = f.readline()
    except FileNotFoundError:
        write_header = True

    with open(out_csv, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            w.writeheader()
        w.writerow({k: row.get(k, "") for k in fieldnames})


def main() -> None:
    """
    Main driver.
    """
    # ----------------------
    # USER INPUTS
    # ----------------------
    celebrity_season_table_csv = "Archit_Preliminary/celebrity_season_premiere_table.csv"
    overrides_csv = "wiki_title_overrides.csv"
    out_csv = "dwts_wiki_edits_preseason.csv"

    # ----------------------
    # Load (season, celebrityname, season_premiere_date) table
    # ----------------------
    table = pd.read_csv(celebrity_season_table_csv)
    season_col = _detect_col(table, ["season"])
    name_col = _detect_col(table, ["celebrity_name", "celebrityname", "celebrity_name", "celebrity"])
    premiere_col = _detect_col(table, ["season_premiere_date", "premiere_date"])
    if not season_col or not name_col or not premiere_col:
        raise ValueError(
            f"{celebrity_season_table_csv} must have columns: season, season_premiere_date, celebrity_name"
        )

    table = table.rename(columns={season_col: "season", name_col: "celebrityname", premiere_col: "season_premiere_date"}).copy()
    table["season"] = pd.to_numeric(table["season"], errors="raise").astype(int)
    table["celebrityname"] = table["celebrityname"].astype(str).str.strip()
    table["season_premiere_date"] = table["season_premiere_date"].astype(str).str.strip()
    table = table.drop_duplicates(subset=["season", "celebrityname"]).sort_values(["season", "celebrityname"], kind="stable")

    overrides = _load_overrides(overrides_csv)
    done = _load_existing_keys(out_csv)

    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": "DWTS-MCM-2026 Wikipedia revisions counter (educational) - contact: your_email@example.com"
        }
    )

    for r in table.itertuples(index=False):
        season = int(r.season)
        celeb_name = str(r.celebrityname)
        premiere = str(r.season_premiere_date)

        key = (season, celeb_name)
        if key in done:
            continue

        if not premiere or premiere.lower() == "nan":
            _append_row(
                out_csv,
                {
                    "season": int(season),
                    "celebrityname": celeb_name,
                    "wiki_title": "",
                    "pageid": "",
                    "cutoff_utc": "",
                    "edits_total_to_cutoff": "",
                    "status": "api_error",
                    "notes": f"Missing season_premiere_date in {celebrity_season_table_csv}",
                },
            )
            done.add(key)
            continue

        cutoff = to_cutoff_utc(premiere)

        # Name -> title heuristic (+ overrides)
        title = overrides.get(celeb_name, celeb_name)

        pageid, normalized_title, is_disambig, status, notes = resolve_pageid(session, title)
        if status != "ok":
            _append_row(
                out_csv,
                {
                    "season": int(season),
                    "celebrityname": celeb_name,
                    "wiki_title": normalized_title,
                    "pageid": pageid if pageid is not None else "",
                    "cutoff_utc": cutoff,
                    "edits_total_to_cutoff": "",
                    "status": status,
                    "notes": notes,
                },
            )
            done.add(key)
            continue

        if is_disambig:
            _append_row(
                out_csv,
                {
                    "season": int(season),
                    "celebrityname": celeb_name,
                    "wiki_title": normalized_title,
                    "pageid": pageid if pageid is not None else "",
                    "cutoff_utc": cutoff,
                    "edits_total_to_cutoff": "",
                    "status": "disambiguation",
                    "notes": "Disambiguation page; add wiki_title_overrides.csv entry to specify correct page",
                },
            )
            done.add(key)
            continue

        # Count revisions
        edits, st2, notes2 = count_revisions_to_cutoff(session, int(pageid), cutoff)
        if st2 != "ok" or edits is None:
            _append_row(
                out_csv,
                {
                    "season": int(season),
                    "celebrityname": celeb_name,
                    "wiki_title": normalized_title,
                    "pageid": int(pageid),
                    "cutoff_utc": cutoff,
                    "edits_total_to_cutoff": "",
                    "status": st2,
                    "notes": notes2,
                },
            )
            done.add(key)
            continue

        _append_row(
            out_csv,
            {
                "season": int(season),
                "celebrityname": celeb_name,
                "wiki_title": normalized_title,
                "pageid": int(pageid),
                "cutoff_utc": cutoff,
                "edits_total_to_cutoff": int(edits),
                "status": "ok",
                "notes": "",
            },
        )
        done.add(key)


if __name__ == "__main__":
    main()


